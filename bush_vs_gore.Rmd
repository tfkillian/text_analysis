---
title: "Bush_v_Gore_2000"
author: "Theo Killian"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
vignette: >
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{depmap}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
suppressPackageStartupMessages(library("dplyr"))
knitr::opts_chunk$set(collapse=TRUE, comment="#>", warning=FALSE, message=FALSE)
```

# Introduction

This report performs a textual analysis of the infamous [Bush v. Gore 2000](https://www.law.cornell.edu/supct/html/00-949.ZPC.html)
opinion. This opinion was written by an anonymous US Supreme Court Justice,
likely by one of the 5 conservative Supreme Court justices serving on the court
at that time: O'Connor, Rehnquist, Thomas, Kennedy and Scalia. In this analysis,
we utilize a number of NLP techniques, such as hierarchical clustering,
bootstrap consensus trees, characteristic vocabulary, and other methods. We have
sampled the opinions written by each of the five conservative justices between
the period 1999-2001 for comparison to the [Bush v. Gore 2000](https://www.law.cornell.edu/supct/html/00-949.ZPC.html)
opinion, in order to ascertain the probable author of this document.

```{r load_libraries}
library("quanteda")
library("dplyr")
library("readtext")
library("ggplot2")
library("tm")
library("gridExtra")
# library("xgboost")
# library("fortunes")
# library("tm")
# library("e1071")
# library("mlr")
set.seed(132)
```

### Load Data

The Bush v. Gore opinion is read into R as a single `readtext` object.  We will
also compile the opinions written by each of the five conservative justices
between the period 1999-2001 (+/- 1 year from when the anonymous SC opinion was
written in 2000). Each body of documents from each justice can be read a single
`readtext` 'glob' where each single document can be individually selected or
evaluated together a single object.

Read the cleaned document feature matrices as *.rds* files from `opinions`
folder. (See the `data_cleaning.Rmd` file for previous data cleaning steps).

```{r}
readRDS("./opinions/dfms/opinion_dfm.rds")
readRDS("./opinions/dfms/kennedy_dfm.rds")
readRDS("./opinions/dfms/oconnor_dfm.rds")
readRDS("./opinions/dfms/rehnquist_dfm.rds")
readRDS("./opinions/dfms/scalia_dfm.rds")
readRDS("./opinions/dfms/thomas_dfm.rds")
```

### Top Features

The plots below show the top 30 terms for each document feature matrix.

```{r top_20, fig.height=14, fig.width=8}
p1 <- textstat_frequency(opinion_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Opinion Piece") +
                theme(plot.title = element_text(hjust = 0.5))

p2 <- textstat_frequency(kennedy_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Kennedy corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p3 <- textstat_frequency(oconnor_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("OConnor corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p4 <- textstat_frequency(rehnquist_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Rehnquist corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p5 <- textstat_frequency(scalia_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Scalia corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p6 <- textstat_frequency(thomas_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Thomas corpus") +
                theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3,
             top = "Top 20 terms for each document corpus")
```

### Comparing Document Similarity by Cosine Distance

Documents are then compared by taking the cosine of the angle between the two
vectors (or the dot product between the normalizations of the two vectors)
formed by any two columns. Values close to 1 represent very similar documents
while values close to 0 represent very dissimilar documents.

```{r}
# https://quanteda.io/reference/textstat_simil.html
cat("Document similarity of Opinion to Kennedy Corpus Documents \n")
textstat_simil(kennedy_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to OConnor Corpus Documents \n")
textstat_simil(oconnor_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Rehnquist Corpus Documents \n")
textstat_simil(rehnquist_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Scalia Corpus Documents \n")
textstat_simil(scalia_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Thomas Corpus Documents \n")
textstat_simil(thomas_dfm, opinion_dfm, method = c("cosine"))
```

The Opinion piece appears closest on average to the Kennedy and OConnor corpus
documents.

### Comparing Document Distance by Cosine Distance

Documents are then compared by taking the cosine of the angle between the two
vectors (or the dot product between the normalizations of the two vectors)
formed by any two columns. Values close to 1 represent very similar documents
while values close to 0 represent very dissimilar documents.

```{r}
cat("Average Document similarity of Opinion to Kennedy Corpus Documents \n")
mean(textstat_simil(kennedy_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to OConnor Corpus Documents \n")
mean(textstat_simil(oconnor_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Rehnquist Corpus Documents \n")
mean(textstat_simil(rehnquist_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Scalia Corpus Documents \n")
mean(textstat_simil(scalia_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Thomas Corpus Documents \n")
mean(textstat_simil(thomas_dfm, opinion_dfm, method = c("cosine"))@x)
```

The Opinion piece appears closest on average to the Rehnquist corpus documents.

```{r}
cat("Average Document distance of Opinion to Kennedy Corpus Documents \n")
mean(textstat_dist(kennedy_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to OConnor Corpus Documents \n")
mean(textstat_dist(oconnor_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Rehnquist Corpus Documents \n")
mean(textstat_dist(rehnquist_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Scalia Corpus Documents \n")
mean(textstat_dist(scalia_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Thomas Corpus Documents \n")
mean(textstat_dist(thomas_dfm, opinion_dfm, method = c("manhattan"))@x)
```

### Word Clouds

#### Opinion Word Cloud

```{r}
textplot_wordcloud(opinion_dfm, max_words = 100)
```

#### Kenndy Word Cloud

```{r}
textplot_wordcloud(kennedy_dfm, max_words = 100)
```

#### O'Connor Word Cloud

```{r}
textplot_wordcloud(oconnor_dfm, max_words = 100)
```

#### Rehnquist Word Cloud

```{r}
textplot_wordcloud(rehnquist_dfm, max_words = 100)
```

#### Scalia Word Cloud

```{r}
textplot_wordcloud(scalia_dfm, max_words = 100)
```

#### Thomas Word Cloud

```{r}
textplot_wordcloud(thomas_dfm, max_words = 100)
```

### Lexical Diversity

`textstat_lexdiv()` calcuates lexical diversity in various measures based on the
number of unique types of tokens and the length of a document. It is useful for
analysing speakers or writers linguistic skill, or complexity of ideas expressed
in documents.

```{r}
opinion_lexdiv <- textstat_lexdiv(opinion_dfm)
kennedy_lexdiv <- textstat_lexdiv(kennedy_dfm)
oconnor_lexdiv <- textstat_lexdiv(oconnor_dfm)
rehnquist_lexdiv <- textstat_lexdiv(rehnquist_dfm)
scalia_lexdiv <- textstat_lexdiv(scalia_dfm)
thomas_lexdiv <- textstat_lexdiv(thomas_dfm)

# head(opinion_lexdiv)
```

The plot below compares the lexical diversity of 

```{r}
par(mfrow=c(3, 2))
plot(kennedy_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR",
     main = "Kennedy Lexical Diversity")
abline(h = opinion_lexdiv$TTR[1], col = "red")
abline(h = mean(kennedy_lexdiv$TTR), col = "blue")

plot(oconnor_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR",
     ylim=c(0.2, 0.5), main = "OConnor Lexical Diversity")
abline(h = opinion_lexdiv$TTR[1], col = "red")
abline(h = mean(oconnor_lexdiv$TTR), col = "blue")

plot(rehnquist_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR",
     main = "Rehnquist Lexical Diversity")
abline(h = opinion_lexdiv$TTR[1], col = "red")
abline(h = mean(rehnquist_lexdiv$TTR), col = "blue")

plot(scalia_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR",
     main = "Scalia Lexical Diversity")
abline(h = opinion_lexdiv$TTR[1], col = "red")
abline(h = mean(scalia_lexdiv$TTR), col = "blue")

plot(thomas_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR",
     main = "Thomas Lexical Diversity")
abline(h = opinion_lexdiv$TTR[1], col = "red")
abline(h = mean(thomas_lexdiv$TTR), col = "blue")
```

### Cluster Dendrogram

`textstat_dist()` calcuates similarites of documents or features for various
measures. Its output is compatible with R's `dist()`, so hierachical clustering
can be perfromed without any transformation.

Below is a dendrogram of all of the documents from all conservative SC justices,
including the Bush v. Gore opinion (can be located in the center).

```{r fig.height=12, fig.width= 18}
# library(dendextend)
all_docs <- bind_rows(opinion, kennedy, oconnor, rehnquist, scalia, thomas)
all_docs <- corpus(all_docs)
all_docs_lower <- tolower(all_docs)
all_docs_toks <- tokens(all_docs_lower)
all_docs_nostop <- tokens_select(all_docs_toks, pattern = stopwords('en'),
                                selection = 'remove')
all_docs_cleaned <- tokens_remove(tokens(all_docs_nostop, remove_punct = TRUE,
                                  remove_numbers = TRUE), stopwords("english"))
all_docs_dfm <- dfm(all_docs_cleaned)
tstat_dist <- as.dist(textstat_dist(all_docs_dfm))
clust <- hclust(tstat_dist)
plot(clust, xlab = "Distance", ylab = NULL, main = "Cluster Dendrogram")

# more info in clustering
# http://mlwiki.org/index.php/Document_Clustering
```


### Selecting Keywords and Keyphrases

You can see how keywords are used in the actual contexts in a concordance view
produced by `kwic()`

```{r}
opinion_kw <- kwic(opinion_corpus, pattern =  'vot*')
head(opinion_kw, 10)
```

You can generate n-grams in any lengths from a tokens using `tokens_ngrams()`. 
N-grams are a sequence of tokens from already tokenized text objects.

```{r}
## where we left off with quanteda
## https://tutorials.quanteda.io/basic-operations/fcm/fcm/
# toks_ngram <- tokens_ngrams(toks, n = 2:4)
```

### TD-IDF

[Term frequency–inverse document frequency (TD-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
is a numerical statistic that is intended to reflect how important a word is to
a document in a collection or corpus. It is often used as a weighting factor in
searches of information retrieval, text mining, and user modeling. The tf–idf
value increases proportionally to the number of times a word appears in the
document and is offset by the number of documents in the corpus that contain the
word, which helps to adjust for the fact that some words appear more frequently
in general. The algorithm works as follows:

* The weight of a term that occurs in a document is simply proportional to the
term frequency. 

* The specificity of a term can be quantified as an inverse function of the
number of documents in which it occurs. 

```{r}
#library("tidytext")
# https://www.tidytextmining.com/tfidf.html

library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

There is one row in this book_words data frame for each word-book combination;
n is the number of times that word is used in that book and total is the total
words in that book. The usual suspects are here with the highest n, "the",
"and", "to"", and so forth. In Figure 3.1, let’s look at the distribution of
n/total for each novel, the number of times a word appears in a novel divided by
the total number of terms (words) in that novel. This is exactly what term
frequency is.

```{r}
library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

There are very long tails to the right for these novels (those extremely rare
words!) that we have not shown in these plots. These plots exhibit similar
distributions for all the novels, with many words that occur rarely and fewer
words that occur frequently.

#### Zipf's Law

Zipf's law states that the frequency that a word appears is inversely
proportional to its rank. 

```{r}
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

The rank column here tells us the rank of each word within the frequency table;
the table was already ordered by n so we could use row_number() to find the
rank. Then, we can calculate the term frequency in the same way we did before.
Zipf’s law is often visualized by plotting rank on the x-axis and term frequency
on the y-axis, on logarithmic scales. Plotting this way, an inversely
proportional relationship will have a constant, negative slope.

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

Notice that Figure 3.2 is in log-log coordinates. We see that all six of Jane
Austen’s novels are similar to each other, and that the relationship between
rank and frequency does have negative slope. It is not quite constant, though;
perhaps we could view this as a broken power law with, say, three sections.
Let’s see what the exponent of the power law is for the middle section of the
rank range.

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

And we have in fact gotten a slope close to -1 here. Let’s plot this fitted
power law with the data in Figure 3.3 to see how it looks.

* frequency is proportional to 1 over *rank* 

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

We have found a result close to the classic version of Zipf’s law for the corpus
of Jane Austen’s novels. The deviations we see here at high rank are not
uncommon for many kinds of language; a corpus of language often contains fewer
rare words than predicted by a single power law. The deviations at low rank are
more unusual. Jane Austen uses a lower percentage of the most common words than
many collections of language. This kind of analysis could be extended to compare
authors, or to compare any other collections of text; it can be implemented
simply using tidy data principles.

```{r}
book_words <- book_words %>%
  bind_tf_idf(word, book, n)

book_words
```

Notice that idf and thus tf-idf are zero for these extremely common words.

```{r}
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
```

### Naive Bayes

```{r}
# https://tutorials.quanteda.io/machine-learning/nb/

```

*Session Info*

```{r sessionInfo}
sessionInfo()
```