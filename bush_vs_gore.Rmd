---
title: "Bush_v_Gore_2000"
author: "Theo Killian"
date: "November 7, 2019"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
vignette: >
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{depmap}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
suppressPackageStartupMessages(library("dplyr"))
knitr::opts_chunk$set(collapse=TRUE, comment="#>", warning=FALSE, message=FALSE)
```

## Introduction

This report performs a textual analysis of the infamous [Bush v. Gore 2000](https://www.law.cornell.edu/supct/html/00-949.ZPC.html)
opinion. This opinion was written by an anonymous US Supreme Court Justice,
likely by one of the 5 conservative Supreme Court justices serving on the court
at that time: O'Connor, Rehnquist, Thomas, Kennedy and Scalia. In this analysis,
we utilize a number of NLP techniques, such as hierarchical clustering,
bootstrap consensus trees, characteristic vocabulary, and other methods. We have
sampled the opinions written by each of the five conservative justices between
the period 1999-2001 for comparison to the [Bush v. Gore 2000](https://www.law.cornell.edu/supct/html/00-949.ZPC.html)
opinion, in order to ascertain the probable author of this document.

```{r load_libraries}
library("readtext")
library("quanteda")
library("dplyr")
library("readtext")
library("ggplot2")
library("tm")
library("gridExtra")
# library("xgboost")
# library("fortunes")
# library("tm")
# library("e1071")
# library("mlr")
```

The Bush v. Gore opinion is read into R as a single `readtext` object.  We will
also compile the opinions written by each of the five conservative justices
between the period 1999-2001 (+/- 1 year from when the anonymous SC opinion was
written in 2000). Each body of documents from each justice can be read a single
`readtext` 'glob' where each single document can be individually selected or
evaluated together a single object.

```{r load_data}
opinion <- readtext("./opinions/Bush_v_Gore.txt")

KENNEDY <- system.file("./opinions/Kennedy/", package = "readtext")
kennedy <- readtext(paste0(KENNEDY, "./opinions/Kennedy/*"))

OCONNOR <- system.file("./opinions/OConnor/", package = "readtext")
oconnor <- readtext(paste0(OCONNOR, "./opinions/OConnor/*"))

REHNQUIST <- system.file("./opinions/Rehnquist/", package = "readtext")
rehnquist <- readtext(paste0(REHNQUIST, "./opinions/Rehnquist/*"))

SCALIA <- system.file("./opinions/Scalia/", package = "readtext")
scalia <- readtext(paste0(SCALIA, "./opinions/Scalia/*"))

THOMAS <- system.file("./opinions/Thomas/", package = "readtext")
thomas <- readtext(paste0(THOMAS, "./opinions/Thomas/*"))
```

### Corpus Generation

[Corpuses](https://tutorials.quanteda.io/basic-operations/corpus/corpus/) for
each SC justice are generated using the `quanteda` R package. A corpus is a data
frame consisting of a character vector for each documents. The summary of each
corpus lists the statistics on `Types,` `Tokens` and `Sentences` for each
document.

```{r corpuses}
opinion_corpus <- corpus(opinion)
kennedy_corpus <- corpus(kennedy)
oconnor_corpus <- corpus(oconnor)
rehnquist_corpus <- corpus(rehnquist)
scalia_corpus <- corpus(scalia)
thomas_corpus <- corpus(thomas)

# summary(opinion_corpus)
# Corpus consisting of 1 document:
# 
#             Text Types Tokens Sentences
#  Bush_v_Gore.txt  1066   4049       168
# summary(kennedy_corpus)
# Corpus consisting of 25 documents:
# 
#                                                                      Text Types Tokens Sentences
#               01 - City of Monterey v Del Monte Dunes at Monterey Ltd.txt  2224  12227       578
#                                                      02 - Glover v US.txt   732   2444       120
#                                    03 - City of West Covina v Perkins.txt   907   3271       159
#                                            04 - INS v Aguirre-Aguirre.txt  1303   6408       309
#                                           05 - US v Haggar Apparel Co.txt  1155   4846       211
#                                                     06 - Peguero v US.txt   650   2536       118
#                                                     07 - US v Johnson.txt   732   2642       113
#                     08 - TrafFix Devices Inc v Marketing Displays Inc.txt  1026   4269       190
#                                            09 - US v United Foods Inc.txt  1087   4020       210
#                                                       10 - US v Locke.txt  1975   9406       447
#                                                     11 - Fischer v US.txt  1208   4985       200
#                                                    12 - Mitchell v US.txt  1388   5827       275
#                                  13 - Legal Services Corp v Velazquez.txt  1306   5165       242
#                                                   14 - Saucier v Katz.txt  1195   5055       227
#  15 - Board of Regents of University of Wisconsin System v Southworth.txt  1469   5984       305
#                                                  16 - Rice v Cayetano.txt  2306  10397       530
#                                                   17 - Garner v Jones.txt  1079   4593       236
#                                            18 - Tuan Anh Nguyen v INS.txt  1465   7039       284
#                                  19 - Circuit City Stores Inc v Adams.txt  1417   6384       268
#                                                 20 - Slack v McDaniel.txt   989   5090       245
#                             21 - US v Playboy Entertainment Group Inc.txt  1903   8539       417
#                  22 - Amoco Production Co v Southern Ute Indian Tribe.txt  1337   5242       234
#                                                23 - Williams v Taylor.txt  1715   8725       388
#                                         24 - Palazzolo v Rhode Island.txt  1919   8602       401
#                                                    25 - Alden v Maine.txt  2933  20241       886
# summary(oconnor_corpus)
# Corpus consisting of 26 documents:
# 
#                                                              Text Types Tokens Sentences
#                   01 - Lackawanna County Dist Attorney v Coss.txt  1054   5039       273
#                                             02 - Daniels v US.txt   925   3661       186
#                       04 - Murphy v United Parcel Service Inc.txt   683   2693       106
#                              05 - US Postal Service v Gregory.txt   808   2938       167
#                            06 - City of Indianapolis v Edmond.txt  1381   5748       324
#                                           07 - Martin v Hadix.txt  1156   6341       278
#                                           08 - Seling v Young.txt  1268   5845       323
#                        09 - Norfolk Southern Ry Co v Shanklin.txt  1036   5346       233
#                                          10 - Penry v Johnson.txt  1361   7539       360
#                          11 - Kolstad v American Dental Ass'n.txt  1610   7441       326
#                            12 - Sutton v United Air Lines Inc.txt  1538   7735       293
#                                          13 - Miller v French.txt  1660   8982       394
#                 14 - Reeves v Sanderson Plumbing Products Inc.txt  1626   7540       366
#                                  15 - Lopez v Monterey County.txt  1476   7782       369
#                       16 - Lewis v Lewis And Clark Marine Inc.txt  1314   6730       369
#                                          17 - Duncan v Walker.txt  1090   5331       233
#                                      18 - Roe v Flores-Ortega.txt  1223   6069       252
#                                  19 - City of Erie v Pap's AM.txt  1698   8116       391
#                                    20 - O'Sullivan v Boerckel.txt   923   3936       205
#                                       21 - Rogers v Tennessee.txt  1345   5969       264
#  22 - Davis Next Friend LaShonda D v Monroe County Bd of Educ.txt  1786   9099       389
#                            23 - Kimel v Florida Bd of Regents.txt  2089  12134       638
#  24 - Food and Drug Admin v Brown And Williamson Tobacco Corp.txt  2456  15412       695
#     25 - Department of Commerce v US House of Representatives.txt  2026  10140       466
#                            26 - Lorillard Tobacco Co v Reilly.txt  2525  15546       840
#          27 - Minnesota v Mille Lacs Band of Chippewa Indians.txt  2133  13684       702
# summary(rehnquist_corpus)
# Corpus consisting of 26 documents:
# 
#                                                                                  Text Types Tokens Sentences
#                                            01 - Department of Army v Blue Fox Inc.txt   938   3685       157
#                                                               02 - Conn v Gabbert.txt   743   2443       124
#                                                                    03 - Bond v US.txt   570   1583        81
#                                          04 - Lujan v G And G Fire Sprinklers Inc.txt   885   3573       182
#                                     05 - Buckman Co v Plaintiffs' Legal Committee.txt  1165   4146       142
#                                                           06 - Illinois v Wardlow.txt   713   2201       138
#                                                                07 - Reno v Condon.txt   905   3424       150
#                                                                 08 - US v Knights.txt   989   3408       168
#                          09 - Los Angeles Police Dept v United Reporting Pub Corp.txt   931   3312       153
#                                                                   10 - Ohler v US.txt   644   2375       104
#                                                               11 - Wilson v Layne.txt  1383   5477       284
#                                                             12 - Florida v Thomas.txt   668   2263       128
#                                         13 - Correctional Services Corp v Malesko.txt  1251   5150       269
#                                                                 14 - Texas v Cobb.txt  1120   4040       197
#                                            15 - Atkinson Trading Co Inc v Shirley.txt  1257   5569       251
#                                                             16 - Weeks v Angelone.txt  1042   4411       208
#  17 - Buckhannon Bd and Care Home Inc v West Virginia Dept of Health and Human Re.txt  1261   5299       258
#                                                                   18 - Neder v US.txt  1741   9683       432
#                                                               19 - Dickerson v US.txt  1520   6537       391
#                                                 20 - Boy Scouts of America v Dale.txt  1552   6898       314
#      21 - Solid Waste Agency of Northern Cook County v US Army Corps of Engineers.txt  1317   5454       235
#                                          22 - American Mfrs Mut Ins Co v Sullivan.txt  1536   7869       364
#                                 23 - Green Tree Financial Corp-Alabama v Randolph.txt  1124   4735       211
#             24 - Florida Prepaid Postsecondary Educ Expense Bd v College Sav Bank.txt  1632   7958       354
#                         25 - Board of Trustees of University of Alabama v Garrett.txt  1582   6329       307
#                                                                26 - US v Morrison.txt  2264  11670       583
# summary(scalia_corpus)
# Corpus consisting of 24 documents:
# 
#                                                                       Text Types Tokens Sentences
#                                                   01 - Artuz v Bennett.txt   716   2479        92
#                   02 - Your Home Visiting Nurse Services Inc v Shalala.txt   776   3022       120
#                                                   03 - New York v Hill.txt  1014   3518       195
#                             05 - Wal-Mart Stores Inc v Samara Bros Inc.txt   989   3886       126
#             06 - Hartford Underwriters Ins Co v Union Planters Bank NA.txt  1253   5098       219
#                    07 - Norfolk Shipbuilding And Drydock Corp v Garris.txt  1043   3889       214
#                          08 - NLRB v Kentucky River Community Care Inc.txt  1347   6317       275
#                                                        09 - Kyllo v US.txt  1319   4975       214
#                                                 10 - Portuondo v Agard.txt  1448   5824       291
#                          11 - Semtek Intern Inc v Lockheed Martin Corp.txt  1162   4789       206
#                                               12 - Edwards v Carpenter.txt   684   2393       100
#          13 - Grupo Mexicano de Desarrollo SA v Alliance Bond Fund Inc.txt  1741   7581       312
#                            14 - US v Sun-Diamond Growers of California.txt  1278   5851       154
#                          15 - Whitman v American Trucking Associations.txt  2048  10802       474
#                                                16 - Wyoming v Houghton.txt  1115   4180       184
#          17 - Grupo Mexicano de Desarrollo SA v Alliance Bond Fund Inc.txt  2027   9696       436
#  18 - College Sav Bank v Florida Prepaid Postsecondary Educ Expense Bd.txt  2140  10115       478
#           19 - Vermont Agency of Natural Resources v US ex rel Stevens.txt  2184  10465       491
#                                   20 - Reno v Bossier Parish School Bd.txt  1574   8149       310
#                                              21 - Alexander v Sandoval.txt  1608   7920       405
#                               22 - California Democratic Party v Jones.txt  1671   7563       342
#                23 - Reno v American-Arab Anti-Discrimination Committee.txt  1753   8384       278
#                                 24 - AT And T Corp v Iowa Utilities Bd.txt  1831  10220       339
#                                                    25 - Nevada v Hicks.txt  1713   8295       359
# summary(thomas_corpus)
# Corpus consisting of 23 documents:
# 
#                                                           Text Types Tokens Sentences
#                                            01 - Jones v US.txt  2044  12766       606
#          02 - Arizona Dept of Revenue v Blaze Const Co Inc.txt   739   2335       140
#                                         03 - Shaw v Murphy.txt   903   3093       173
#                                       04 - Florida v White.txt   705   2479       125
#                                 05 - US v Rodriguez-Moreno.txt   705   2778       114
#          06 - Director of Revenue of Missouri v CoBank ACB.txt   670   2936       116
#                07 - Pollard v EI du Pont de Nemours And Co.txt   931   3987       158
#                     08 - Cunningham v Hamilton County Ohio.txt  1243   4789       264
#                                          09 - Sims v Apfel.txt   976   3587       200
#                   10 - US v Oakland Cannabis Buyers' Co-op.txt  1250   5728       257
#                                         11 - Beck v Prupis.txt  1211   5736       218
#                                            12 - Baral v US.txt   742   3448       106
#                    13 - Egelhoff v Egelhoff ex rel Breiner.txt  1053   3975       199
#                                      14 - Hunt v Cromartie.txt  1245   5068       256
#                                         15 - Gitlitz v CIR.txt   861   5312       185
#        16 - JEM Ag Supply Inc v Pioneer Hi-Bred Intern Inc.txt  1554   7794       366
#                                           17 - Carter v US.txt  1501   7479       323
#                                          18 - Tyler v Cain.txt  1000   4926       240
#                           19 - Christensen v Harris County.txt  1056   4602       197
#                         20 - Hughes Aircraft Co v Jacobson.txt  1240   5632       248
#  21 - Harris Trust and Sav Bank v Salomon Smith Barney Inc.txt  1215   6284       200
#               22 - Good News Club v Milford Central School.txt  1555   7730       364
#                                       23 - Smith v Robbins.txt  1953  11778       518
```

### Data Cleaning

We will need to convert the text to lower case and remove any non-ASCII
characters so that the texts can be compared for later analysis. The corpuses
will be tokenized, English stop words (such as "the", "and" etc.) removed and
the corpuses converted to document feature matrices (DFMs).

```{r}
## convert the corpus to all lower case
opinion_lower <- tolower(opinion_corpus)
kennedy_lower <- tolower(kennedy_corpus)
oconnor_lower <- tolower(oconnor_corpus)
rehnquist_lower <- tolower(rehnquist_corpus)
scalia_lower <- tolower(scalia_corpus)
thomas_lower <- tolower(thomas_corpus)

## convert the coprpus into tokens
opinion_tokes <- tokens(opinion_lower)
kennedy_tokes <- tokens(kennedy_lower)
oconnor_tokes <- tokens(oconnor_lower)
rehnquist_tokes <- tokens(rehnquist_lower)
scalia_tokes <- tokens(scalia_lower)
thomas_tokes <- tokens(thomas_lower)

## remove stop words from corpus
opinion_nostop <- tokens_select(opinion_tokes, pattern = stopwords('en'),
                                selection = 'remove')
kennedy_nostop <- tokens_select(kennedy_tokes, pattern = stopwords('en'),
                                selection = 'remove')
oconnor_nostop <- tokens_select(oconnor_tokes, pattern = stopwords('en'),
                                selection = 'remove')
rehnquist_nostop <- tokens_select(rehnquist_tokes, pattern = stopwords('en'),
                                selection = 'remove')
scalia_nostop <- tokens_select(scalia_tokes, pattern = stopwords('en'),
                                selection = 'remove')
thomas_nostop <- tokens_select(thomas_tokes, pattern = stopwords('en'),
                                selection = 'remove')

## remove numbers and punctuation from corpus
opinion_cleaned <- tokens_remove(tokens(opinion_nostop, remove_punct = TRUE,
                                 remove_numbers = TRUE), stopwords("english"))
kennedy_cleaned <- tokens_remove(tokens(kennedy_nostop, remove_punct = TRUE,
                                 remove_numbers = TRUE), stopwords("english"))
oconnor_cleaned <- tokens_remove(tokens(oconnor_nostop, remove_punct = TRUE,
                                 remove_numbers = TRUE), stopwords("english"))
rehnquist_cleaned <- tokens_remove(tokens(rehnquist_nostop, remove_punct = TRUE,
                                   remove_numbers = TRUE), stopwords("english"))
scalia_cleaned <- tokens_remove(tokens(scalia_nostop, remove_punct = TRUE,
                                 remove_numbers = TRUE), stopwords("english"))
thomas_cleaned <- tokens_remove(tokens(thomas_nostop, remove_punct = TRUE,
                                remove_numbers = TRUE), stopwords("english"))

## Create the corpus dfm
opinion_dfm <- dfm(opinion_cleaned)
kennedy_dfm <- dfm(kennedy_cleaned)
oconnor_dfm <- dfm(oconnor_cleaned)
rehnquist_dfm <- dfm(rehnquist_cleaned)
scalia_dfm <- dfm(scalia_cleaned)
thomas_dfm <- dfm(thomas_cleaned)

## It is possible to remove features such as redundant words in a DFM, however a
## FSM must be constructed in order for the terms to be selected and removed.

# opinion_dfm <- dfm_remove(opinion_dfm, pattern = c("gore", "counties"))
# dfmat_news <- dfm_trim(dfmat_news, min_termfreq = 100)

## Below shows a summary of each document feature matrix (dfm)

# opinion_dfm
# 
# kennedy_dfm
# 
# oconnor_dfm
# 
# rehnquist_dfm
# 
# scalia_dfm
# 
# thomas_dfm

## top features
# topfeatures(opinion_dfm, 100)
# 
# topfeatures(kennedy_dfm, 100)
# 
# topfeatures(oconnor_dfm, 100)
# 
# topfeatures(rehnquist_dfm, 100)
# 
# topfeatures(scalia_dfm, 100)
# 
# topfeatures(thomas_dfm, 100)
```

### Top Features

The plots below show the top 30 terms for each document feature matrix.

```{r top_20, fig.height=14, fig.width=8}
p1 <- textstat_frequency(opinion_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Opinion Piece") +
                theme(plot.title = element_text(hjust = 0.5))

p2 <- textstat_frequency(kennedy_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Kennedy corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p3 <- textstat_frequency(oconnor_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("OConnor corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p4 <- textstat_frequency(rehnquist_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Rehnquist corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p5 <- textstat_frequency(scalia_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Scalia corpus") +
                theme(plot.title = element_text(hjust = 0.5))

p6 <- textstat_frequency(thomas_dfm) %>%
                dplyr::select(feature, frequency) %>%
                top_n(20) %>%
                ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
                geom_point() +
                coord_flip() +
                labs(x = NULL, y = "Frequency") +
                theme_minimal() +
                ggtitle("Thomas corpus") +
                theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3,
             top = "Top 20 terms for each document corpus")
```

<!-- ### Construct the LSA model -->

<!-- [Latent semantic analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis) -->
<!-- is a technique in natural language processing -->
<!-- (NLP), in particular distributional semantics, of analyzing relationships -->
<!-- between a set of documents and the terms they contain by producing a set of -->
<!-- concepts related to the documents and terms. LSA assumes that words that are -->
<!-- close in meaning will occur in similar pieces of text (the distributional -->
<!-- hypothesis). A matrix containing word counts per document (rows represent unique -->
<!-- words and columns represent each document) is constructed from a large piece of -->
<!-- text and a mathematical technique called singular value decomposition (SVD) is -->
<!-- used to reduce the number of rows while preserving the similarity structure -->
<!-- among columns. -->

```{r}
## construction LSA models of the corpus DFMs
# kennedy_lsa <- textmodel_lsa(kennedy_dfm)
# oconnor_lsa <- textmodel_lsa(oconnor_dfm)
# rehnquist_lsa <- textmodel_lsa(rehnquist_dfm)
# scalia_lsa <- textmodel_lsa(scalia_dfm)
# thomas_lsa <- textmodel_lsa(thomas_dfm)
```

<!-- Now that LSA models have been constructed for each corpus, the can be applied -->
<!-- to the Opinion piece. The distance of this piece can be represented in the -->
<!-- reduced 2-dimensional space. -->

### Comparing Document Similarity

Documents are then compared by taking the cosine of the angle between the two
vectors (or the dot product between the normalizations of the two vectors)
formed by any two columns. Values close to 1 represent very similar documents
while values close to 0 represent very dissimilar documents.

<!-- ```{r} -->
<!-- # https://quanteda.io/articles/pkgdown/examples/lsa.html -->
<!-- # querydfm1 <- opinion_dfm %>% dfm_select(pattern = kennedy_dfm) -->
<!-- # newq1 <- predict(kennedy_lsa, newdata = querydfm1) -->
<!-- # newq1$docs_newspace[, 1:2] -->
<!-- #  -->
<!-- # querydfm2 <- opinion_dfm %>% dfm_select(pattern = oconnor_dfm) -->
<!-- # newq2 <- predict(oconnor_lsa, newdata = querydfm2) -->
<!-- # newq2$docs_newspace[, 1:2] -->
<!-- #  -->
<!-- # querydfm3 <- opinion_dfm %>% dfm_select(pattern = rehnquist_dfm) -->
<!-- # newq3 <- predict(rehnquist_lsa, newdata = querydfm3) -->
<!-- # newq3$docs_newspace[, 1:2] -->
<!-- #  -->
<!-- # querydfm3 <- opinion_dfm %>% dfm_select(pattern = rehnquist_dfm) -->
<!-- # newq3 <- predict(rehnquist_lsa, newdata = querydfm3) -->
<!-- # newq3$docs_newspace[, 1:2] -->
<!-- #  -->
<!-- # querydfm4 <- opinion_dfm %>% dfm_select(pattern = scalia_dfm) -->
<!-- # newq4 <- predict(scalia_lsa, newdata = querydfm4) -->
<!-- # newq4$docs_newspace[, 1:2] -->
<!-- #  -->
<!-- # querydfm5 <- opinion_dfm %>% dfm_select(pattern = thomas_dfm) -->
<!-- # newq5 <- predict(thomas_lsa, newdata = querydfm5) -->
<!-- # newq5$docs_newspace[, 1:2] -->
<!-- ``` -->

```{r}
# https://quanteda.io/reference/textstat_simil.html
cat("Document similarity of Opinion to Kennedy Corpus Documents \n")
textstat_simil(kennedy_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to OConnor Corpus Documents \n")
textstat_simil(oconnor_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Rehnquist Corpus Documents \n")
textstat_simil(rehnquist_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Scalia Corpus Documents \n")
textstat_simil(scalia_dfm, opinion_dfm, method = c("cosine"))
```

```{r}
cat("Document similarity of Opinion to Thomas Corpus Documents \n")
textstat_simil(thomas_dfm, opinion_dfm, method = c("cosine"))
```

The Opinion piece appears closest on average to the Kennedy and OConnor corpus
documents.

```{r}
cat("Average Document similarity of Opinion to Kennedy Corpus Documents \n")
mean(textstat_simil(kennedy_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to OConnor Corpus Documents \n")
mean(textstat_simil(oconnor_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Rehnquist Corpus Documents \n")
mean(textstat_simil(rehnquist_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Scalia Corpus Documents \n")
mean(textstat_simil(scalia_dfm, opinion_dfm, method = c("cosine"))@x)
cat("Average Document similarity of Opinion to Thomas Corpus Documents \n")
mean(textstat_simil(thomas_dfm, opinion_dfm, method = c("cosine"))@x)
```

The Opinion piece appears closest on average to the Rehnquist corpus documents.

```{r}
cat("Average Document distance of Opinion to Kennedy Corpus Documents \n")
mean(textstat_dist(kennedy_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to OConnor Corpus Documents \n")
mean(textstat_dist(oconnor_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Rehnquist Corpus Documents \n")
mean(textstat_dist(rehnquist_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Scalia Corpus Documents \n")
mean(textstat_dist(scalia_dfm, opinion_dfm, method = c("manhattan"))@x)
cat("Average Document distance of Opinion to Thomas Corpus Documents \n")
mean(textstat_dist(thomas_dfm, opinion_dfm, method = c("manhattan"))@x)
```

You can see how keywords are used in the actual contexts in a concordance view
produced by `kwic()`

```{r}
# kw_immig <- kwic(toks, pattern =  'immig*')
# head(kw_immig, 10)
```

You can generate n-grams in any lengths from a tokens using `tokens_ngrams()`. 
N-grams are a sequence of tokens from already tokenized text objects.

```{r}
# toks_ngram <- tokens_ngrams(toks, n = 2:4)
```

### Naive Bayes

```{r}
# #naive bayes
# library("e1071")
```

<!-- ### Random Forest -->

<!-- ```{r} -->
<!-- # # https://www.displayr.com/text-analysis-hooking-up-your-term-document-matrix-to-custom-r-code/ -->
<!-- # # devtools::install_github("Displayr/flipMultivariates") -->
<!-- # library(flipMultivariates) # Our package containing the Random Forest routine -->
<!-- # library(tm) # The package needed to convert the sparse matrix -->
<!-- # tdm <- as.matrix(term.document.matrix) # Convert the sparse matrix before use -->
<!-- # colnames(tdm) <- make.names(colnames(tdm)) # Ensure the column names are appropriate for use in an R model -->
<!-- # df <- data.frame(TweetSource = tweetSource, tdm) # Combine the outcome variable with the term document matrix -->
<!-- # f <- formula(paste0("TweetSource ~ ", paste0(colnames(tdm), collapse = "+"))) # Create the R Formula which describes the relationship we are interrogating -->
<!-- # rf <- RandomForest(f, df) # Run the random forest model -->
<!-- ``` -->


<!-- ### Stylometric Analysis -->

<!-- #### Hierarchical cluster analysis -->

<!-- A first statistical technique we can apply to this data set is hierarchical -->
<!-- cluster analysis. The goal of this procedure is to generate a tree structure or -->
<!-- dendrogram, which will help to identify the most important groups or clusters of -->
<!-- texts in the data. For this procedure we will first calculate the distance -->
<!-- between corpuses of opinions of the five conservative Supreme Court justices and -->
<!-- the anonymous Bush v. Gore opinion. To calculate these distances, we combine can -->
<!-- apply a distance metric to the rows in the frequency table described above like, -->
<!-- for instance, the well-known Euclidean distance metric. The measure of distance -->
<!-- is an important tool in statistical analysis. It quantifies dissimilarity -->
<!-- between sample data for numerical computation. A popular choice of distance -->
<!-- metric is the Euclidean distance, which is the square root of sum of squares of -->
<!-- attribute differences. We will use Burrows's Delta, a well-known distance metric -->
<!-- in stylometry, which has been reported to work well in a variety of text -->
<!-- analysis tasks in stylometry.  -->

<!-- Such a technique is useful to estimate the stylistic distance (hence `Delta`) -->
<!-- between two texts and can be used as the input for various clustering -->
<!-- algorithms. When comparing the frequency difference for a word in two texts, -->
<!-- the Delta metric will also take into account the average fluctuations in that -->
<!-- word's frequency in the other texts in the corpus. The resulting score, "Delta", -->
<!-- can therefore give us an idea of the stylistic dissimilarity between two texts: -->
<!-- the larger Delta, the more dissimilar the texts are from a stylistic point of -->
<!-- view. Based on the resulting distances, we can now build a dendrogram in a -->
<!-- bottom-up fashion, so to speak, starting at the leafs, representing the -->
<!-- individual texts in our case. We begin by joining the two texts which had the -->
<!-- lowest Delta score, and then we combine these into a new node at a slightly -->
<!-- higher level in the tree. Next, we combine the two texts (or a text and one of -->
<!-- the newly formed nodes) that had the second lowest Delta and we also combine -->
<!-- these in a new node. Then the third node, etc., until all nodes have eventually -->
<!-- been combined into a single top node. A example of the kind tree structure which -->
<!-- can be build in such a way is offered in Figure 1, where we have carried out a -->
<!-- cluster analysis on the frequencies of the 500 words which were most frequent -->
<!-- throughout the corpus. -->

<!-- ```{r} -->
<!-- # euclidean distance measure -->
<!-- # https://rstudio-pubs-static.s3.amazonaws.com/266040_d2920f956b9d4bd296e6464a5ccc92a1.html -->
<!-- # d <- dist(as.matrix(mtcars)) -->
<!-- # hc <- hclust(d)                # apply hirarchical clustering  -->
<!-- # plot(hc) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # unsupervised hierarchical cluster analysis -->
<!-- # may want to rename the .txt files to be SCJ - case.txt so the authorship is -->
<!-- # obvious when clustered in a tree -->

<!-- ``` -->

<!-- It deserves emphasis that this form of cluster analysis is completely -->
<!-- "unsupervised": it has no pre-conceived ideas about the data or any potential -->
<!-- groupings in it. The analysis only has access to the lexical frequency -->
<!-- information about the texts and is not guided by the researcher to a specific -->
<!-- hypothesis. Unfortunately, cluster analyses have reported to yield somewhat -->
<!-- unstable dendrograms, heavily dependent on the number of words that are for -->
<!-- instance analyzed and a number of other technical parameters which are beyond -->
<!-- the scope of the paper. Therefore, it is common nowadays in stylometry to -->
<!-- complement cluster experiments with Bootstrap Consensus Trees (BCT). -->

<!-- In this iterative procedure, we run a series of cluster analyses, each time -->
<!-- inspecting different frequency bands, starting for instance with the band of the -->
<!-- 1-100 most frequent words (MFW), then the 50-150 MFW, 100-200 MFW and so on, -->
<!-- until all frequency bands have been analyzed. The results of the cluster -->
<!-- analyses can then be combined in a "consensus tree." In this representation, we -->
<!-- collapse cluster nodes that are not observed in at least 50% of the analyses. -->
<!-- While BCT and similar procedures have known successful applications in the study -->
<!-- of language history, the technique has also recently been ported to -->
<!-- computational stylistics. 34 It is interesting for text analysis, because it can -->
<!-- test for the robustness of stylistic similarities across different frequency -->
<!-- bands and thus can give more solid results. -->

<!-- ```{r} -->
<!-- # bootstrap consensus trees -->
<!-- # library(ape) -->
<!-- # library(MASS) -->
<!-- # set.seed(123) -->
<!-- #  -->
<!-- # tr <- rcoal(10) -->
<!-- #  -->
<!-- # # 'evolve' copy number down the tree using Brownian motion -->
<!-- # cn_matrix <- ceiling( t( mvrnorm(100, mu=rep(3,10), Sigma=vcv(tr)))) -->
<!-- #  -->
<!-- # # Then all you need to do is create a function to estimate your tree with the -->
<!-- # # matrix as input. Here's the simplest one: -->
<!-- # estimate_tr <- function(m) nj(dist(m, method="manhattan")) -->
<!-- # point_est <- estimate_tr(cn_matrix) -->
<!-- # bs <- boot.phylo(point_est, cn_matrix, estimate_tr, trees=TRUE)   -->
<!-- # con <- consensus(bs$trees, p=0.5) -->

<!-- # https://cran.r-project.org/web/packages/phangorn/vignettes/IntertwiningTreesAndNetworks.html -->
<!-- ``` -->

<!-- ### Characteristic Vocabulary -->

<!-- It would be interesting to find out which lexical items a computational analysis -->
<!-- is to single out as most characteristic for each cluster. To this end, we will -->
<!-- make use of a "parsimonious language model"" or PLM. This technique stems from -->
<!-- the field of Informational Retrieval, which deals for instance with the study of -->
<!-- search engines like Yahoo or Google search. Using PLMs, search engines try to -->
<!-- determine which vocabulary is most typical of a given text amidst a corpus of -->
<!-- other texts. Thus, a PLM tries to estimate for each document in a collection, -->
<!-- which words a user would be most likely to use as query terms, when they are -->
<!-- searching for this specific document. A user is, for instance, unlikely to use -->
<!-- a highly common function word to retrieve a specific document, just like they -->
<!-- are unlikely to use an uncommon spelling error in a document which is also -->
<!-- extremely infrequent in other texts. PLMs try to come up with a "model" for each -->
<!-- document that tries to capture these probabilities and trades off between the -->
<!-- most likely and most unlikely words in a document. -->

<!-- ```{r 100_words_opinion} -->
<!-- # most common words in each corpus -->
<!-- ``` -->

<!-- ```{r 100_words_kennedy} -->

<!-- ``` -->

<!-- ```{r 100_words_oconnor} -->

<!-- ``` -->

<!-- ```{r 100_words_rehnquist} -->

<!-- ``` -->

<!-- ```{r 100_words_scalia} -->

<!-- ``` -->

<!-- ```{r 100_words_thomas} -->

<!-- ``` -->


<!-- ### Other -->

<!-- ```{r} -->
<!-- # #Create the dfm: -->
<!-- # dfm_bvg <- dfm(tokes) -->
<!-- # View(dfm_bvg) -->
<!-- #  -->
<!-- # # Remove sparse features? -->
<!-- #  -->
<!-- # # some sort of similarity measure -->
<!-- # #http://text2vec.org/similarity.html#cosine_similarity_with_tf-idf -->
<!-- # library("text2vec") -->
<!-- #  -->
<!-- # #naive bayes -->
<!-- # library("e1071") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # #generate n-grams in any lengths from a tokens using tokens_ngrams(). N-grams -->
<!-- # are a sequence of #tokens from already tokenized text objects. -->
<!-- #  -->
<!-- # ngram <- tokens_ngrams(toks, n = 2:4) -->
<!-- # head(ngram[[1]], 50) -->
<!-- # #ngram -->
<!-- #  -->
<!-- # #Selective ngrams -->
<!-- # #While tokens_ngrams() generates n-grams or skip-grams in all possible  -->
<!-- # #combinations of tokens, tokens_compound() generates n-grams more selectively.  -->
<!-- # #For example, you can make negation bi-grams using phrase() and a wild card (*). -->
<!-- #  -->
<!-- # neg_bigram <- tokens_compound(toks, pattern = phrase('not *')) -->
<!-- # neg_bigram <- tokens_select(neg_bigram, pattern = phrase('not_*')) -->
<!-- # head(neg_bigram[[1]], 50) -->
<!-- #  -->
<!-- # #tokens_ngrans() is an efficient function, but it returns a large object if  -->
<!-- # #multiple values are given to n or skip. Since n-grams inflates the size of  -->
<!-- # #objects without adding much information, we recommend to generate n-grams more  -->
<!-- # #selectively using tokens_compound(). -->
<!-- #  -->
<!-- # #Document Feature Matrix -->
<!-- # #https://www.r-bloggers.com/who-wrote-that-anonymous-nyt-op-ed-text-similarity-analyses-with-r/ -->
<!-- # #https://www.rjionline.org/stories/we-put-data-science-to-the-test-to-try-to-uncover-the-mystery-author-of-the -->
<!-- # #https://github.com/mkearney/resist_oped -->
<!-- # #https://en.wikipedia.org/wiki/Document-term_matrix -->
<!-- #  -->
<!-- # require(fortunes) -->
<!-- # require(tm) -->
<!-- # sentences <- NULL -->
<!-- # for (i in 1:10) sentences <- c(sentences,fortune(i)$quote) -->
<!-- # d <- data.frame(textCol =sentences ) -->
<!-- # ds <- DataframeSource(d) -->
<!-- # dsc<-Corpus(ds) -->
<!-- # dtm<- DocumentTermMatrix(dsc, control = list(weighting = weightTf, stopwords = TRUE)) -->
<!-- # dictC <- Dictionary(dtm) -->
<!-- # # The query below is created from words in fortune(1) and fortune(2) -->
<!-- # newQry <- data.frame(textCol = "lets stand up and be counted seems to work undocumented") -->
<!-- # newQryC <- Corpus(DataframeSource(newQry)) -->
<!-- # dtmNewQry <- DocumentTermMatrix(newQryC, control = list(weighting=weightTf,stopwords=TRUE,dictionary=dict1)) -->
<!-- # dictQry <- Dictionary(dtmNewQry) -->
<!-- # # Below does a naive similarity (number of features in common) -->
<!-- # apply(dtm,1,function(x,y=dictQry){length(intersect(names(x)[x!= 0],y))}) -->
<!-- #  -->
<!-- # #Transforming Text convert to a data frame and then to a corpus. -->
<!-- # df <- do.call("rbind", lapply(rdmTweets, as.data.frame)) -->
<!-- # dim(df) -->
<!-- #  -->
<!-- # library(tm) -->
<!-- # # build a corpus, which is a collection of text documents -->
<!-- # # VectorSource specifies that the source is character vectors. -->
<!-- # myCorpus <- Corpus(VectorSource(df$text)) -->
<!-- #  -->
<!-- # #The corpus needs a couple of transformations, including changing letters to  -->
<!-- # #lower case, removing punctuations/numbers and removing stop words. The general  -->
<!-- # #English stop-word list is tailored by adding "available" and "via" and removing "r". -->
<!-- # myCorpus <- tm_map(myCorpus, tolower) -->
<!-- #  -->
<!-- # # remove punctuation -->
<!-- # myCorpus <- tm_map(myCorpus, removePunctuation) -->
<!-- # # remove numbers -->
<!-- # myCorpus <- tm_map(myCorpus, removeNumbers) -->
<!-- # # remove stopwords -->
<!-- # # keep "r" by removing it from stopwords -->
<!-- # myStopwords <- c(stopwords("english"), "available", "via") -->
<!-- # idx <- which(myStopwords == "r") -->
<!-- # myStopwords <- myStopwords[-idx] -->
<!-- # myCorpus <- tm_map(myCorpus, removeWords, myStopwords) -->
<!-- #  -->
<!-- # #Stemming Words -->
<!-- #  -->
<!-- # #In many cases, words need to be stemmed to retrieve their radicals. For  -->
<!-- # #instance, "example" and "examples" are both stemmed to "exampl". However, after -->
<!-- # #that, one may want to complete the stems to their original forms, so that the  -->
<!-- # #words would look "normal". -->
<!-- #  -->
<!-- # dictCorpus <- myCorpus -->
<!-- # # stem words in a text document with the snowball stemmers, -->
<!-- # # which requires packages Snowball, RWeka, rJava, RWekajars -->
<!-- # myCorpus <- tm_map(myCorpus, stemDocument) -->
<!-- # # inspect the first three "documents" -->
<!-- # inspect(myCorpus[1:3]) -->
<!-- # # stem completion -->
<!-- # myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus) -->
<!-- #  -->
<!-- # #Print the first three documents in the built corpus. -->
<!-- # inspect(myCorpus[1:3]) -->
<!-- #  -->
<!-- # #Something unexpected in the above stemming and stem completion is that, word  -->
<!-- # #"mining" is first stemmed to "mine", and then is completed to "miners", instead -->
<!-- # #of "mining", although there are many instances of "mining" in the tweets,  -->
<!-- # #compared to only one instance of "miners". -->
<!-- #  -->
<!-- # #Building a Document-Term Matrix -->
<!-- #  -->
<!-- # myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1)) -->
<!-- # inspect(myDtm[266:270,31:40]) -->
<!-- # #A term-document matrix (5 terms, 10 documents) -->
<!-- # #Non-/sparse entries: 9/41 -->
<!-- # #Sparsity : 82% -->
<!-- # #Maximal term length: 12 -->
<!-- # #Weighting : term frequency (tf) -->
<!-- #  -->
<!-- # #Frequent Terms and Associations -->
<!-- #  -->
<!-- # findFreqTerms(myDtm, lowfreq=10) -->
<!-- #  -->
<!-- # # which words are associated with "r"? -->
<!-- # findAssocs(myDtm, 'r', 0.30) -->
<!-- #  -->
<!-- # #a <- tm_map(a, removeWords, stopwords("english")) -->
<!-- #  -->
<!-- # #Popular document embedding and similarity measures: -->
<!-- #  -->
<!-- # #Doc2Vec -->
<!-- # #Average w2v vectors -->
<!-- # #Weighted average w2v vectors (e.g. tf-idf) -->
<!-- # #RNN-based embeddings (e.g. deep LSTM networks) -->
<!-- #  -->
<!-- # #Latent Semantic Indexing -->
<!-- # #https://medium.com/@adriensieg/text-similarities-da019229c894 -->
<!-- #  -->
<!-- # #random forest  -->
<!-- #  -->
<!-- # #naive bayes -->
<!-- #  -->
<!-- # #Getting started with Naive Bayes -->
<!-- # #Install the package -->
<!-- # #install.packages("e1071") -->
<!-- #  -->
<!-- # #Loading the library -->
<!-- # library(e1071) -->
<!-- # ?naiveBayes #The documentation also contains an example implementation of Titanic dataset -->
<!-- #  -->
<!-- # #Next load the Titanic dataset -->
<!-- # data("Titanic") -->
<!-- #  -->
<!-- # #Save into a data frame and view it -->
<!-- # Titanic_df=as.data.frame(Titanic) -->
<!-- #  -->
<!-- # #Creating data from table -->
<!-- # repeating_sequence=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq)  -->
<!-- # #This will repeat each combination equal to the frequency of each combination -->
<!-- #  -->
<!-- # #Create the dataset by row repetition created -->
<!-- # Titanic_dataset=Titanic_df[repeating_sequence,] -->
<!-- #  -->
<!-- # #We no longer need the frequency, drop the feature -->
<!-- # Titanic_dataset$Freq=NULL -->
<!-- #  -->
<!-- # #The data is now ready for Naive Bayes to process. Let's fit the model -->
<!-- # #Fitting the Naive Bayes model -->
<!-- # Naive_Bayes_Model=naiveBayes(Survived ~., data=Titanic_dataset) -->
<!-- # #What does the model say? Print the model summary -->
<!-- # Naive_Bayes_Model -->
<!-- #  -->
<!-- # #Call: -->
<!-- # #    naiveBayes.default(x = X, y = Y, laplace = laplace) -->
<!-- #  -->
<!-- # #A-priori probabilities: -->
<!-- # #    Y -->
<!-- # #No      Yes  -->
<!-- # #0.676965 0.323035  -->
<!-- #  -->
<!-- # #Conditional probabilities: -->
<!-- # #    Class -->
<!-- # #Y          1st          2nd         3rd         Crew -->
<!-- # #No    0.08187919  0.11208054  0.35436242  0.45167785 -->
<!-- # #Yes   0.28551336  0.16596343  0.25035162  0.29817159 -->
<!-- #  -->
<!-- # #Sex -->
<!-- # #Y          Male         Female -->
<!-- # #No    0.91543624  0.08456376 -->
<!-- # #Yes   0.51617440  0.48382560 -->
<!-- #  -->
<!-- # #Age -->
<!-- # #Y         Child         Adult -->
<!-- # #No    0.03489933  0.96510067 -->
<!-- # #Yes   0.08016878  0.91983122 -->
<!-- #  -->
<!-- # #Prediction on the dataset -->
<!-- # NB_Predictions=predict(Naive_Bayes_Model,Titanic_dataset) -->
<!-- # #Confusion matrix to check accuracy -->
<!-- # table(NB_Predictions,Titanic_dataset$Survived) -->
<!-- # #NB_Predictions      No      Yes -->
<!-- # #No      1364    362 -->
<!-- # #Yes     126     349 -->
<!-- #  -->
<!-- # #Getting started with Naive Bayes in mlr -->
<!-- # #Install the package -->
<!-- # #install.packages("mlr") -->
<!-- # #Loading the library -->
<!-- # library(mlr) -->
<!-- #  -->
<!-- # #Create a classification task for learning on Titanic Dataset and specify the target feature -->
<!-- # task = makeClassifTask(data = Titanic_dataset, target = "Survived") -->
<!-- # #Initialize the Naive Bayes classifier -->
<!-- # selected_model = makeLearner("classif.naiveBayes") -->
<!-- # #Train the model -->
<!-- # NB_mlr = train(selected_model, task) -->
<!-- #  -->
<!-- # #Read the model learned   -->
<!-- # NB_mlr$learner.model -->
<!-- # #Naive Bayes Classifier for Discrete Predictors -->
<!-- #  -->
<!-- # #Call: -->
<!-- # #    naiveBayes.default(x = X, y = Y, laplace = laplace) -->
<!-- #  -->
<!-- # #A-priori probabilities: -->
<!-- # #    Y -->
<!-- # #No      Yes  -->
<!-- # #0.676965 0.323035  -->
<!-- #  -->
<!-- # #Conditional probabilities: -->
<!-- # #    Class -->
<!-- # #Y               1st         2nd         3rd         Crew -->
<!-- # #No      0.08187919  0.11208054  0.35436242  0.45167785 -->
<!-- # #Yes     0.28551336  0.16596343  0.25035162  0.29817159 -->
<!-- #  -->
<!-- # #Sex -->
<!-- # #Y               Male        Female -->
<!-- # #No     0.91543624  0.08456376 -->
<!-- # #Yes     0.51617440  0.48382560 -->
<!-- #  -->
<!-- # #Age -->
<!-- # #Y           Child       Adult -->
<!-- # #No      0.03489933  0.96510067 -->
<!-- # #Yes     0.08016878  0.91983122 -->
<!-- #  -->
<!-- # #Predict on the dataset without passing the target feature -->
<!-- # predictions_mlr = as.data.frame(predict(NB_mlr, newdata = Titanic_dataset[,1:3])) -->
<!-- #  -->
<!-- # ##Confusion matrix to check accuracy -->
<!-- # table(predictions_mlr[,1],Titanic_dataset$Survived) -->
<!-- # #No      Yes -->
<!-- # #No    1364    362 -->
<!-- # #Yes   126     349 -->
<!-- #  -->
<!-- # #svm  -->

<!-- #CNN  -->
<!-- ``` -->

```{r sessionInfo}
sessionInfo()
```